{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b5e99f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset, WeightedRandomSampler\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from ax.plot.contour import plot_contour\n",
    "from ax.plot.trace import optimization_trace_single_method\n",
    "from ax.service.managed_loop import optimize\n",
    "from ax.utils.notebook.plotting import render\n",
    "from ax.utils.tutorials.cnn_utils import train, evaluate\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31fa127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_path = 'skin-lesions/train/'\n",
    "data_valid_path = 'skin-lesions/valid/'\n",
    "data_test_path = 'skin-lesions/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff284f2d",
   "metadata": {},
   "source": [
    "https://discuss.pytorch.org/t/is-there-a-limit-on-how-disbalanced-a-train-set-can-be/26334/6?u=ptrblck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66354f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.RandomHorizontalFlip(p = 0.5),\n",
    "#     transforms.RandomRotation(degrees = 20),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "# ])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = torch.load('model_data/train_dataset.pt')\n",
    "# labels = []\n",
    "# for idx, (sample, target) in enumerate(tqdm(train_dataset, total=len(train_dataset))):\n",
    "#     labels.append(int(target))\n",
    "# cls_weights = torch.from_numpy(\n",
    "#     compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "# )\n",
    "\n",
    "# weights = cls_weights[labels]\n",
    "# sampler = WeightedRandomSampler(weights, len(labels), replacement=True)\n",
    "\n",
    "# train_dataset = ImageFolder(data_train_path, transform = train_transform)\n",
    "\n",
    "valid_dataset = ImageFolder(data_valid_path, transform = test_transform)\n",
    "test_dataset = ImageFolder(data_test_path, transform = test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size = 32, shuffle = True)\n",
    "test_loader = DataLoader(test_dataset, batch_size = 1, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a28bd56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d489cb91f4fa494aebcdd19edf27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modified_outputs, labels = [], []\n",
    "for idx, (sample, target) in enumerate(tqdm(train_dataset, total=len(train_dataset))):\n",
    "    modified_outputs.append(sample.cpu().detach().numpy())\n",
    "    labels.append(target)\n",
    "\n",
    "X_train, y_train = np.array(modified_outputs), np.array(labels)\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "\n",
    "train_rows=len(X_train)\n",
    "X_train = X_train.reshape(train_rows,-1)\n",
    "\n",
    "X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "X_train = X_train.reshape(-1, 3, 224, 224)\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
    "train_loader = DataLoader(train_dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5162b128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, criterion, min_loss, optimizer, vectorize=False):\n",
    "    training_losses, valid_losses, accs = [],[],[]\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0\n",
    "        model.train()\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            if vectorize:\n",
    "                images = images.reshape(-1, 224 * 224)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            ps = model(images)\n",
    "            labels = labels.reshape(len(labels), 1)\n",
    "            loss = criterion(ps, labels.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss.item()\n",
    "        print(f\"\\tEPOCH: {epoch + 1}.. TRAINING LOSS: {training_loss}\")\n",
    "\n",
    "        training_losses.append(training_loss)\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        acc = 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in valid_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if vectorize:\n",
    "                    images = images.reshape(-1, 224 * 224 * 3)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                ps = model(images)\n",
    "                labels = labels.reshape(len(labels), 1)\n",
    "                loss = criterion(ps, labels.float())\n",
    "                \n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "                _, top_class = ps.topk(1, dim = 1)\n",
    "                eq = top_class == labels.view(-1, 1)\n",
    "                acc += eq.sum().item()\n",
    "                \n",
    "        valid_losses.append(valid_loss)\n",
    "        accs.append(acc)\n",
    "        acc = (acc/len(valid_dataset)) * 100\n",
    "        print(\"EPOCHS: {}/{}.. \\tTRAINING LOSS: {:.6f}.. \\tVALIDATION LOSS: {:.6f}.. \\tACCURACY: {:.2f}%..\".format(epoch + 1, epochs, training_loss, valid_loss, acc))\n",
    "        \n",
    "        if valid_loss <= min_loss:\n",
    "            print(\"Saving Model {:.4f} ---> {:.4f}\".format(min_loss, valid_loss))\n",
    "            save_obj = OrderedDict([\n",
    "                (\"min_loss\", valid_loss),\n",
    "                (\"model\", model.state_dict())\n",
    "            ])\n",
    "            torch.save(save_obj, \"/melanoma_model.pt\")\n",
    "            min_loss = valid_loss\n",
    "            \n",
    "    return training_losses, valid_losses, accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40b151af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LogisticRegression, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(224 * 224 * 3, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, 224 * 224 * 3)\n",
    "        outputs = self.model(x)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "299855d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(parameterization):\n",
    "\n",
    "    model = LogisticRegression().to(device)\n",
    "\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False # Freeze feature extractor\n",
    "        \n",
    "    return model # return untrained model\n",
    "\n",
    "\n",
    "def net_train(net, train_loader, parameters, dtype, device):\n",
    "    net.to(dtype=dtype, device=device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), # or any optimizer you prefer \n",
    "                        lr=parameters.get(\"lr\", 0.001), # 0.001 is used if no lr is specified\n",
    "                        momentum=parameters.get(\"momentum\", 0.9)\n",
    "    )\n",
    "\n",
    "    scheduler = optim.lr_scheduler.StepLR(\n",
    "      optimizer,\n",
    "      step_size=int(parameters.get(\"step_size\", 30)),\n",
    "      gamma=parameters.get(\"gamma\", 1.0),  # default is no learning rate decay\n",
    "    )\n",
    "\n",
    "    num_epochs = parameters.get(\"num_epochs\", 3) # Play around with epoch number\n",
    "    # Train Network\n",
    "    for _ in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            # move data to proper dtype and device\n",
    "            inputs = inputs.to(dtype=dtype, device=device)\n",
    "            inputs = inputs.reshape(-1, 224 * 224 * 3)\n",
    "            \n",
    "            labels = labels.to(device=device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.requires_grad=True\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "        return net\n",
    "\n",
    "\n",
    "def train_evaluate(parameterization):\n",
    "\n",
    "    # constructing a new training data loader allows us to tune the batch size\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                batch_size=parameterization.get(\"batchsize\", 32),\n",
    "                                sampler=sampler)\n",
    "        \n",
    "    # Get neural net\n",
    "    untrained_net = init_net(parameterization)\n",
    "    \n",
    "    # train\n",
    "    trained_net = net_train(net=untrained_net, train_loader=train_loader, \n",
    "                            parameters=parameterization, dtype=torch.float, device=device)\n",
    "    \n",
    "    # return the accuracy of the model as it was trained in this run\n",
    "    return evaluate(\n",
    "        net=trained_net,\n",
    "        data_loader=test_loader,\n",
    "        dtype=dtype,\n",
    "        device=device,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f072ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters, values, experiment, model = optimize(\n",
    "    parameters=[\n",
    "        {\"name\": \"lr\", \"type\": \"range\", \"bounds\": [1e-6, 0.4], \"log_scale\": True},\n",
    "        {\"name\": \"batchsize\", \"type\": \"range\", \"bounds\": [16, 128]},\n",
    "        {\"name\": \"momentum\", \"type\": \"range\", \"bounds\": [0.0, 1.0]},\n",
    "#         {\"name\": \"max_epoch\", \"type\": \"range\", \"bounds\": [1, 30]},\n",
    "#         {\"name\": \"stepsize\", \"type\": \"range\", \"bounds\": [20, 40]},        \n",
    "    ],\n",
    "  \n",
    "    evaluation_function=train_evaluate,\n",
    "    objective_name='accuracy',\n",
    ")\n",
    "\n",
    "print(best_parameters)\n",
    "means, covariances = values\n",
    "print(means)\n",
    "print(covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa756ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEPOCH: 1.. TRAINING LOSS: 35.40567845106125\n",
      "EPOCHS: 1/50.. \tTRAINING LOSS: 35.405678.. \tVALIDATION LOSS: 3.092602.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 2.. TRAINING LOSS: 35.27925366163254\n",
      "EPOCHS: 2/50.. \tTRAINING LOSS: 35.279254.. \tVALIDATION LOSS: 3.789863.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 3.. TRAINING LOSS: 35.3453808426857\n",
      "EPOCHS: 3/50.. \tTRAINING LOSS: 35.345381.. \tVALIDATION LOSS: 2.685122.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 4.. TRAINING LOSS: 34.82768899202347\n",
      "EPOCHS: 4/50.. \tTRAINING LOSS: 34.827689.. \tVALIDATION LOSS: 3.526606.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 5.. TRAINING LOSS: 34.64899832010269\n",
      "EPOCHS: 5/50.. \tTRAINING LOSS: 34.648998.. \tVALIDATION LOSS: 3.059412.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 6.. TRAINING LOSS: 34.57590037584305\n",
      "EPOCHS: 6/50.. \tTRAINING LOSS: 34.575900.. \tVALIDATION LOSS: 2.644466.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 7.. TRAINING LOSS: 34.493152499198914\n",
      "EPOCHS: 7/50.. \tTRAINING LOSS: 34.493152.. \tVALIDATION LOSS: 4.349686.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 8.. TRAINING LOSS: 34.24269276857376\n",
      "EPOCHS: 8/50.. \tTRAINING LOSS: 34.242693.. \tVALIDATION LOSS: 3.973423.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 9.. TRAINING LOSS: 34.16496515274048\n",
      "EPOCHS: 9/50.. \tTRAINING LOSS: 34.164965.. \tVALIDATION LOSS: 4.744960.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 10.. TRAINING LOSS: 34.18468987941742\n",
      "EPOCHS: 10/50.. \tTRAINING LOSS: 34.184690.. \tVALIDATION LOSS: 4.881824.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 11.. TRAINING LOSS: 34.187712013721466\n",
      "EPOCHS: 11/50.. \tTRAINING LOSS: 34.187712.. \tVALIDATION LOSS: 3.991940.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 12.. TRAINING LOSS: 34.620981991291046\n",
      "EPOCHS: 12/50.. \tTRAINING LOSS: 34.620982.. \tVALIDATION LOSS: 3.241966.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 13.. TRAINING LOSS: 34.01850926876068\n",
      "EPOCHS: 13/50.. \tTRAINING LOSS: 34.018509.. \tVALIDATION LOSS: 2.824712.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 14.. TRAINING LOSS: 33.89097821712494\n",
      "EPOCHS: 14/50.. \tTRAINING LOSS: 33.890978.. \tVALIDATION LOSS: 4.659276.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 15.. TRAINING LOSS: 33.53600937128067\n",
      "EPOCHS: 15/50.. \tTRAINING LOSS: 33.536009.. \tVALIDATION LOSS: 4.249146.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 16.. TRAINING LOSS: 34.21546655893326\n",
      "EPOCHS: 16/50.. \tTRAINING LOSS: 34.215467.. \tVALIDATION LOSS: 6.331241.. \tACCURACY: 20.00%..\n",
      "\tEPOCH: 17.. TRAINING LOSS: 33.828449726104736\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m     10\u001b[0m min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 11\u001b[0m train_loss, valid_loss, accs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvectorize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, epochs, criterion, min_loss, optimizer, vectorize)\u001b[0m\n\u001b[1;32m     25\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m valid_loader:\n\u001b[1;32m     28\u001b[0m         images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m vectorize:\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torch/utils/data/dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[0;32m--> 530\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    533\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    534\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torch/utils/data/dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    569\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    572\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data)\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfetch\u001b[39m(\u001b[38;5;28mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torchvision/datasets/folder.py:230\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    229\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 230\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torchvision/datasets/folder.py:269\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/torchvision/datasets/folder.py:249\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    248\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/PIL/Image.py:889\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    891\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    893\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/anom_detect/lib/python3.9/site-packages/PIL/ImageFile.py:253\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    249\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    252\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 253\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = LogisticRegression().to(device)\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EPOCHS = 50\n",
    "optimizer = optim.SGD(model.parameters(), lr = LEARNING_RATE)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "min_loss = -1\n",
    "train_loss, valid_loss, accs = train(model, EPOCHS, criterion, min_loss, optimizer, vectorize=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
